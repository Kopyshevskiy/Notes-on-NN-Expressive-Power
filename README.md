# Notes on the Expressive Power of Neural Networks

This repository contains a set of LaTeX notes on the theoretical foundations of neural networks. The notes cover key concepts such as the Universal Approximation Theorem, the role of activation functions, and the complexity of shallow vs. deep networks.

## Key Topics Covered

- **Sigmoidal and Discriminatory Functions**: Formal definitions and their importance as activation functions.
- **The Universal Approximation Theorem**: A proof sketch using the Hahn-Banach and Riesz Representation theorems to show why shallow networks are universal approximators.
- **Role of ReLU**: A proof that the ReLU function is 1-discriminatory.
- **Complexity of Neural Networks**: Theorems comparing the number of neurons required for shallow vs. deep networks to approximate functions, highlighting the benefits of depth in avoiding the curse of dimensionality.
