# Notes on the Expressive Power of Neural Networks

This repository contains a set of LaTeX notes on the theoretical foundations of neural networks. The notes cover key concepts such as the Universal Approximation Theorem, the role of activation functions, and the complexity of shallow vs. deep networks.

The content is compiled from the theoretical sections of a Jupyter Notebook and aims to provide a clear, self-contained document for study and reference.

## Key Topics Covered

- **Sigmoidal and Discriminatory Functions**: Formal definitions and their importance as activation functions.
- **The Universal Approximation Theorem**: A proof sketch using the Hahn-Banach and Riesz Representation theorems to show why shallow networks are universal approximators.
- **Role of ReLU**: A proof that the ReLU function is 1-discriminatory.
- **Complexity of Neural Networks**: Theorems comparing the number of neurons required for shallow vs. deep networks to approximate functions, highlighting the benefits of depth in avoiding the curse of dimensionality.
